{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Browsing Agent\n",
    "This notebook will use the internet to download content like pdfs. Then an LLM will decide which files to keep. Next it will create a vector index of the files, for easy RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from open_agent import OpenAgent\n",
    "from config import Config\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import requests\n",
    "from googlesearch import search\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import regex as re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for PDF URLs using a query\n",
    "def search_pdf_urls(query, num_results=10):\n",
    "    pdf_urls = []\n",
    "    for url in search(query, num_results=num_results):\n",
    "        # Optionally check if the URL really ends with '.pdf'\n",
    "        if url.lower().endswith(\".pdf\"):\n",
    "            pdf_urls.append(url)\n",
    "    return pdf_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download a PDF file from a given URL\n",
    "def download_pdf(url, dest_folder=\"downloads\"):\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "    local_filename = os.path.join(dest_folder, url.split(\"/\")[-1])\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(local_filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        print(f\"Downloaded: {local_filename}\")\n",
    "        return local_filename\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract metadata (e.g., creation date) from the PDF\n",
    "def extract_pdf_metadata(pdf_path):\n",
    "    metadata = {}\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            metadata = reader.metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from the first few pages of the PDF\n",
    "def extract_pdf_text(pdf_path, max_pages=3):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(reader.pages)\n",
    "            pages_to_read = min(num_pages, max_pages)\n",
    "            for i in range(pages_to_read):\n",
    "                page = reader.pages[i]\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OpenAgent(api_key=Config.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pdf(pdf_text, context):\n",
    "    # Analyze the pdf text together with the context.\n",
    "    # Next we try to see if it is relevant to the context or not\n",
    "    # Build our XML prompt\n",
    "    prompt = f\"\"\"\n",
    "        <prompt>\n",
    "            <objective>Given the following document text as extracted from a PDF, decide if it is relevant to the context or not</objective>\n",
    "            <instuctions>\n",
    "                <instruction>\n",
    "                    Analyze the document text in the section \"document_text\" and the context in the section \"context\" and decide if the document is relevant to the context or not.\n",
    "                </instruction>\n",
    "                <instruction>\n",
    "                    It is important that the brand name is the same in the document and the context.\n",
    "                </instruction>\n",
    "                <instruction>\n",
    "                    Answer with \"True\" if the document is relevant to the context, and \"False\" if it is not.\n",
    "                </instruction>\n",
    "            </instuctions>\n",
    "            <document_text>\n",
    "                {pdf_text}\n",
    "            </document_text>\n",
    "            <context>\n",
    "                {context}   \n",
    "            </context>\n",
    "\n",
    "        </prompt>\"\"\"\n",
    "    \n",
    "    # Send the prompt to the OpenAI API\n",
    "    response = agent.chat(text=prompt)\n",
    "    # Try to extract the response as True or False\n",
    "    try:\n",
    "        output = response.lower()\n",
    "        if output == \"true\":\n",
    "            return True\n",
    "        elif output == \"false\":\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting response: {e}\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = \"mölnycke mepilex border flex\"\n",
    "#brand = \"mölnycke Mepilex Border Heel\"\n",
    "#brand = \"Essity Libero Touch\"\n",
    "product_type = \"product sheet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the search query. The filetype operator helps to target PDFs.\n",
    "query = f\"{brand} {product_type} filetype:pdf\"\n",
    "print(\"Searching for PDFs...\")\n",
    "pdf_urls = search_pdf_urls(query, num_results=10)\n",
    "print(f\"Found {len(pdf_urls)} PDF URLs.\")\n",
    "\n",
    "relevant_pdfs = []\n",
    "# Process each found PDF\n",
    "for url in pdf_urls:\n",
    "    print(f\"\\nProcessing URL: {url}\")\n",
    "    file_path = download_pdf(url)\n",
    "    if not file_path:\n",
    "        continue\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the pdf files from the downloads folder\n",
    "folder = \"downloads\"\n",
    "# append the folder to the file path\n",
    "pdf_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
    "pdf_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the PDFs\n",
    "See if the data is valid for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = \"Mölnlycke\"\n",
    "#company = \"Essity\"\n",
    "context = f\"We want to match a product with the brand name: {brand} to the product sheet (pdf). The manufacturer should be {company}. The pdf should be a product sheet.\"\n",
    "print(context)\n",
    "relevant_pdfs = []\n",
    "for file_path in pdf_files:\n",
    "    #print(f'Analyzing PDF: {file_path}')\n",
    "    # Extract metadata such as creation date\n",
    "    # metadata = extract_pdf_metadata(file_path)\n",
    "    # creation_date = metadata.get(\"/CreationDate\", \"Unknown\")\n",
    "    # print(f\"Creation Date (from metadata): {creation_date}\")\n",
    "\n",
    "    # Extract text from the PDF \n",
    "    pdf_text = extract_pdf_text(file_path, max_pages=10)\n",
    "\n",
    "    # See if the PDF is relevant to the context\n",
    "    is_relevant = analyze_pdf(pdf_text, context)\n",
    "    print(f\"{file_path} Is Relevant: {is_relevant}\")\n",
    "    if is_relevant:\n",
    "        relevant_pdfs.append(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the data\n",
    "Ask questions about the data using the PDFs and an LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the full text of all relevant pdfs and add them in a RAG response\n",
    "main_query = 'Extract the available sizes from the product sheet, only answer with information that belongs to Mepilex Border Flex. Answer with a table.'\n",
    "#main_query = 'Extract the available sizes from the product sheet together with their arcticle number. Answer with a table.'\n",
    "\n",
    "# Iterate over the relevant PDFs and update the query\n",
    "\n",
    "\n",
    "files_xml = \"\"\n",
    "files_xml = \"\"\n",
    "for file_path in relevant_pdfs:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_content = extract_pdf_text(file_path)\n",
    "    # Create an XML-like structure for each file.\n",
    "    files_xml += f\"<file>\\n  <name>{file_name}</name>\\n  <file_content>{file_content}</file_content>\\n</file>\\n\"\n",
    "\n",
    "\n",
    "query = f\"\"\"\n",
    "    <prompt>\n",
    "        <objective>Answer the main query by extracting the information from the product sheets. The main quary is in \"main_query\" and the content is in \"content\". Each file has its own \"file\" where \"name\" is the name of the pdf file, the \"file_content\" is the actual pdf text.</objective>\n",
    "        <main_query>\n",
    "            {main_query}\n",
    "        </main_query>\n",
    "        <instruction>\n",
    "            For each piece of extracted information, provide the source file name.\n",
    "        </instruction>\n",
    "        <content>\n",
    "            {files_xml}\n",
    "        </content>\n",
    "    </prompt>\n",
    "\"\"\"\n",
    "\n",
    "response = agent.chat(text=query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a vector index of the relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'hello world 42'\n",
    "embedding = agent.get_embedding(text)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = len(embedding)\n",
    "# Create a faiss index\n",
    "index = faiss.IndexFlatIP(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_length=500):\n",
    "    \"\"\"\n",
    "    Splits text into chunks of up to max_length characters.\n",
    "    This is a simple splitter that uses sentence boundaries.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= max_length:\n",
    "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text, file_name):\n",
    "    # Split the text into chunks\n",
    "    chunks = chunk_text(text)\n",
    "    # Embed each chunk\n",
    "    embeddings = []\n",
    "    documents = [] #The documents for each chunk\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if chunk:\n",
    "            embedding = agent.get_embedding(chunk)\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "            documents.append({\n",
    "                            \"file\": file_name,\n",
    "                            \"chunk_index\": i,\n",
    "                            \"text\": chunk\n",
    "                        })\n",
    "    return embeddings, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, documents = embed_text(pdf_text, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the embeddings to the index\n",
    "embeddings = np.array(embeddings)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Available Sizes\"\n",
    "# Embed the query\n",
    "query_embedding = agent.get_embedding(query)\n",
    "# Search the index\n",
    "k = 3\n",
    "D, I = index.search(np.array([query_embedding]), k)\n",
    "# Display the search results\n",
    "for i in range(k):\n",
    "    print(f\"Result {i+1}\")\n",
    "    print(f\"Distance: {D[0][i]}\")\n",
    "    print(f\"Document: {documents[I[0][i]]['file']}\")\n",
    "    print(f\"Chunk Index: {documents[I[0][i]]['chunk_index']}\")\n",
    "    print(f\"Text: {documents[I[0][i]]['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_pdfs = pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
